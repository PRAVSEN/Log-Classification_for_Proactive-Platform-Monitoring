# üîç Log Classification for Proactive Platform Monitoring

## üìò Project Overview

This capstone project focuses on building a machine learning pipeline to classify system log messages into categories such as `error` and `info`. The goal is to proactively alert platform engineers about potential issues before they escalate, enabling faster response and improved system reliability.

---

## üß† Problem Statement

Modern platforms generate thousands of log messages daily. Manually inspecting these logs is inefficient and error-prone. This project automates log classification to identify critical messages (`error`) and distinguish them from routine ones (`info`), helping engineers detect brewing issues early.

---

## üìä Exploratory Data Analysis (EDA)

### üìÅ Dataset Overview

The dataset comprises log entries collected from AWS EKS (Elastic Kubernetes Service) across various services and platform components. These logs are continuously generated in real time, reflecting system activity and usage patterns. For this project, a representative subset of logs was extracted over a defined time window to serve as the basis for analysis. The primary objective is to proactively identify error messages that warrant immediate attention and to detect anomalies or irregularities that may indicate underlying system issues requiring intervention.

- **Rows**: 28,971
- **Columns**: 4
- **Fields**:
  - `Time`: timestamp of log entry
  - `service_name`: source of the log
  - `detected_level`: log severity (e.g., info, error)
  - `log`: unstructured log message text

### ‚úÖ Missing Values
- No missing values detected across any columns.
- **Key Takeaway**: Dataset is complete and clean.

### üî¢ Data Types Distribution
- `object`: text-based columns
- `int64`: numeric identifiers and timestamps
- **Key Takeaway**: Majority of columns are text-based, followed by numeric types.

### üßÆ Service Names by Detected Level
- Grouped bar chart shows distribution of `info`, `error`, and `debug` logs across services.
- **Key Takeaway**: Error-level logs are concentrated in specific services‚Äîpotential system hotspots.

### üî§ Frequent Words in Logs
- Tokenized `log` column and calculated word frequencies.
- Visualized using bar chart and word cloud.
- **Key Takeaway**: Common terms include `timeout`, `connection`, `error`, suggesting recurring system issues.

### ‚è∞ Log Messages by Hour
- Converted timestamps from UTC to EST (UTC‚àí5).
- Extracted hourly distribution of logs.
- **Key Takeaway**: Most logs are generated after business hours‚Äîlikely due to ETL batch operations.

### üìè Log Message Length & Frequency
- Most log messages are under 500 bytes.
- **Key Takeaway**: ERROR logs tend to be longer due to stack traces, while INFO logs are typically concise.

---

## üß† Clustering & Embedding - Unsupervised Learning

To better understand the structure and semantics of system log messages, this project incorporates unsupervised learning techniques such as **clustering** and **embedding**.

### üéØ Objective

- **Discover Hidden Patterns**  
  Group similar log messages to uncover recurring themes, potential issues, or unexpected anomalies without relying on labeled data.

- **Dimensionality Reduction & Visualization**  
  Use embedding techniques (e.g., TF-IDF, Word2Vec, or transformer-based models) to convert raw text into numerical vectors for easier analysis and visualization.

- **Anomaly Detection**  
  Identify outlier clusters that may represent rare or abnormal log entries‚Äîpotential indicators of system faults or security concerns.

- **Feature Engineering for Classification**  
  Generate meaningful vector representations of log messages to serve as input features for downstream supervised classification models.

### üìê TF-IDF Embeddings
- Applied TF-IDF vectorization to extract semantic patterns.
- 2D projection shows consistent clustering between train and test sets.

### üîç Clustering Techniques - Pattern & Anomaly Detection
- Applied DBSCAN and KMeans to reduced embeddings.
- DBScan clustering did perform well on identifying Anomaly Detections when applied on larger datasets
- **Key Takeaway**: Both methods generalize well across splits, capturing consistent structure.

---

## ‚öñÔ∏è Scaling & KDE Visualization

### ‚úÖ Scaling Best Practices
- Label encoding applied only to target variable
- Train-test split performed before scaling
- Avoided scaling one-hot encoded categorical features

### üìà Importance of Scaling
- **StandardScaler** centers SVD components to mean 0 and scales to unit variance.
- Applied only to continuous features (SVD components).
- Prevents data leakage and preserves categorical meaning.

### üìä KDE Output Summary
- **Before Scaling**: SVD components show natural separation but varied ranges.
- **After Scaling**: Distributions are centered and standardized.
- **Key Takeaway**: KDE confirms scaling preserves class separation while normalizing feature behavior.

---

## üîß Pipeline Summary

### 1. Data Preparation
- Loaded logs from `Log-Classification_Capstone_Final.csv`
- Filtered for `error` and `info`
- Encoded target labels using `LabelEncoder`

### 2. Feature Engineering
- **Text**: TF-IDF vectorization
- **Time**: Extracted hour from timestamp
- **Service**: One-hot encoded `service_name`
- **Dimensionality Reduction**: Truncated SVD (3 components)
- **Scaling**: StandardScaler applied to SVD components only

### 3. Modeling - Supervised Learning
- Train/test split (80/20) with stratified sampling
- Combined scaled SVD + unscaled metadata
- Trained classifiers with `GridSearchCV`:
  - Logistic Regression
  - K-Nearest Neighbors
  - Decision Tree
  - Random Forest
  - Support Vector Machine (Linear Kernel)
- Included Dummy Classifier as baseline

### 4. Evaluation
- ‚úÖ Cross-validated predictions (`cross_val_predict`)
- ‚úÖ Confusion matrix visualization
- ‚úÖ ROC-AUC curves for model discrimination
- ‚úÖ Precision, recall, F1-score, and support metrics

---

# üìä Baseline Model Evaluation

This section summarizes the performance of five machine learning classifiers trained to distinguish between `error` and `info` log messages. The evaluation was conducted on a held-out test set using precision, recall, and F1-score as key metrics.

---

## ‚úÖ Model Performance Summary

| Model               | Precision | Recall | F1-Score | Support |
|--------------------|-----------|--------|----------|---------|
| Logistic Regression| 0.9993    | 0.9993 | 0.9993   | 5795    |
| K-Nearest Neighbors| 0.9998    | 0.9998 | 0.9998   | 5795    |
| Decision Tree      | 0.9991    | 0.9991 | 0.9991   | 5795    |
| Random Forest      | 0.9995    | 0.9995 | 0.9995   | 5795    |
| Support Vector Machine| 0.9995 | 0.9995 | 0.9995   | 5795    |

---

## üß† Key Takeaway

All models achieved near-perfect scores across all metrics, indicating extremely strong performance on the test set. This suggests that:

- The feature engineering pipeline (TF-IDF, SVD, metadata fusion) is highly effective.
- The classification task may be relatively easy given the current dataset.
- Adding more logs mainly **"info"** category made the dataset more **imbalanced** and the precision/recalls scores went down to 83%
  - Random Forest was able to handle class imbalances when combined with class weighting parameter
  - Gradient Boosting models that handle Class Imbalances applying hyperparameter tuning using scale_pos_weight on minority subsets were identified

---

## ‚ö†Ô∏è Recommendation

To ensure these results generalize to real-world logs:

- Validate with cross-validation or a separate holdout set.
- Test on unseen or noisy logs from production environments.
- Monitor for alert fatigue if models over-predict `error`.


## Confusion Matrix & Metric Analysis

| Metric        | Description                                                                 |
|---------------|-----------------------------------------------------------------------------|
| **Accuracy**  | Overall correctness of predictions                                          |
| **Precision** | % of predicted `error` logs that are truly errors (controls false positives)|
| **Recall**    | % of actual `error` logs correctly identified (controls false negatives)    |
| **F1-score**  | Harmonic mean of precision and recall                                       |

### Why FP and FN Matter

- **False Positives (FP)**: Info logs misclassified as errors  
  ‚Üí May cause alert fatigue or unnecessary investigations

- **False Negatives (FN)**: Error logs misclassified as info  
  ‚Üí **Critical risk**: Engineers miss real issues, leading to downtime or failures

### Priority: **Recall > Precision > Accuracy**
In this use case, **recall is most important**. Missing an actual error (FN) is far more dangerous than over-alerting (FP). The system must prioritize catching every potential issue‚Äîeven if it means a few false alarms.

---

## üèÅ Future Enhancements

This project demonstrates a scalable approach to log classification, enabling proactive platform monitoring. Future enhancements may include:

- Infuse more Log data with more unique and random cases, to identify the effectiveness of the models
- Infuse random error data to ensure the models identify the errors that are anomalies and not being trained upon
- Real-time log ingestion and classification
- Integration with alerting systems
- Deployment as a microservice for production use
